{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WDx9YDwTarLp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk import PorterStemmer, WordNetLemmatizer\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "vWlSgD0TayNm",
        "outputId": "596e7523-c250-4730-c354-2a26acf0dc0a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>cyberbullying_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In other words #katandandre, your food was cra...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          tweet_text cyberbullying_type\n",
              "0  In other words #katandandre, your food was cra...  not_cyberbullying\n",
              "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
              "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
              "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
              "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(r'C:\\Users\\DELL\\Desktop\\project\\dataset\\cyberbullying_tweets.csv')\n",
        "data.head()\n",
        "data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH_cCepuayO1",
        "outputId": "b2c0f6df-61a2-4d23-f147-4b30f6778a73"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "cyberbullying_type   cyberbullying_type_encoded\n",
              "religion             5                             7998\n",
              "age                  0                             7992\n",
              "gender               2                             7973\n",
              "ethnicity            1                             7961\n",
              "not_cyberbullying    3                             7945\n",
              "other_cyberbullying  4                             7823\n",
              "dtype: int64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labelencoder = LabelEncoder()\n",
        "data['cyberbullying_type_encoded'] = labelencoder.fit_transform(data['cyberbullying_type'])\n",
        "data[['cyberbullying_type', 'cyberbullying_type_encoded']].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IrbocJaqayTf"
      },
      "outputs": [],
      "source": [
        "# preprocessing functions\n",
        "\n",
        "# converting tweet text to lower case\n",
        "def text_lower(text):\n",
        "    return text.str.lower()\n",
        "\n",
        "# removing stopwoords from the tweet text\n",
        "def clean_stopwords(text):\n",
        "    # stopwords list that needs to be excluded from the data\n",
        "    stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
        "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
        "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
        "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
        "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
        "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
        "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
        "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
        "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
        "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
        "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
        "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
        "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
        "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
        "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
        "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']\n",
        "    STOPWORDS = set(stopwordlist)\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "# cleaning and removing punctuations\n",
        "def clean_puctuations(text):\n",
        "    english_puctuations = string.punctuation\n",
        "    translator = str.maketrans('','', english_puctuations)\n",
        "    return text.translate(translator)\n",
        "\n",
        "# cleaning and removing repeating characters\n",
        "def clean_repeating_characters(text):\n",
        "    return re.sub(r'(.)1+', r'1', text)\n",
        "\n",
        "# cleaning and removing URLs\n",
        "def clean_URLs(text):\n",
        "    return re.sub(r\"((www.[^s]+)|(http\\S+))\",\"\",text)\n",
        "\n",
        "# cleaning and removing numeric data\n",
        "def clean_numeric(text):\n",
        "    return re.sub('[0-9]+', '', text)\n",
        "\n",
        "# Tokenization of tweet text\n",
        "def tokenize_tweet(text):\n",
        "    tokenizer = RegexpTokenizer('\\w+')\n",
        "    text = text.apply(tokenizer.tokenize)\n",
        "    return text\n",
        "\n",
        "# stemming    \n",
        "def text_stemming(text):\n",
        "    st = PorterStemmer()\n",
        "    text = [st.stem(word) for word in text]\n",
        "    return text\n",
        "\n",
        "# lemmatization\n",
        "def text_lemmatization(text):\n",
        "    lm = WordNetLemmatizer()\n",
        "    text = [lm.lemmatize(word) for word in text]\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "aRyxn5ZebA24",
        "outputId": "02d9dfbd-17a9-4c47-b743-fc8151a901e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading wordnet: <urlopen error [WinError 10060] A\n",
            "[nltk_data]     connection attempt failed because the connected party\n",
            "[nltk_data]     did not properly respond after a period of time, or\n",
            "[nltk_data]     established connection failed because connected host\n",
            "[nltk_data]     has failed to respond>\n",
            "[nltk_data] Error loading omw-1.4: <urlopen error [WinError 10060] A\n",
            "[nltk_data]     connection attempt failed because the connected party\n",
            "[nltk_data]     did not properly respond after a period of time, or\n",
            "[nltk_data]     established connection failed because connected host\n",
            "[nltk_data]     has failed to respond>\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>cyberbullying_type</th>\n",
              "      <th>cyberbullying_type_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>word katandandr food crapilici mkr</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aussietv white mkr theblock imacelebrityau tod...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>xochitlsuckkk classi whore red velvet cupcak</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>jasongio meh p thank head up but not concern a...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>rudhoeenglish isi account pretend kurdish acco...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47687</th>\n",
              "      <td>black ppl arent expect anyth depend anyth yet ...</td>\n",
              "      <td>ethnicity</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47688</th>\n",
              "      <td>turner not withhold disappoint turner call cou...</td>\n",
              "      <td>ethnicity</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47689</th>\n",
              "      <td>swear god dumb nigger bitch got bleach hair re...</td>\n",
              "      <td>ethnicity</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47690</th>\n",
              "      <td>yea fuck rt therealexel nigger fuck unfollow m...</td>\n",
              "      <td>ethnicity</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47691</th>\n",
              "      <td>bro u gotta chill rt chillshrammi dog fuck kp ...</td>\n",
              "      <td>ethnicity</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>47692 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              tweet_text cyberbullying_type  \\\n",
              "0                     word katandandr food crapilici mkr  not_cyberbullying   \n",
              "1      aussietv white mkr theblock imacelebrityau tod...  not_cyberbullying   \n",
              "2           xochitlsuckkk classi whore red velvet cupcak  not_cyberbullying   \n",
              "3      jasongio meh p thank head up but not concern a...  not_cyberbullying   \n",
              "4      rudhoeenglish isi account pretend kurdish acco...  not_cyberbullying   \n",
              "...                                                  ...                ...   \n",
              "47687  black ppl arent expect anyth depend anyth yet ...          ethnicity   \n",
              "47688  turner not withhold disappoint turner call cou...          ethnicity   \n",
              "47689  swear god dumb nigger bitch got bleach hair re...          ethnicity   \n",
              "47690  yea fuck rt therealexel nigger fuck unfollow m...          ethnicity   \n",
              "47691  bro u gotta chill rt chillshrammi dog fuck kp ...          ethnicity   \n",
              "\n",
              "       cyberbullying_type_encoded  \n",
              "0                               3  \n",
              "1                               3  \n",
              "2                               3  \n",
              "3                               3  \n",
              "4                               3  \n",
              "...                           ...  \n",
              "47687                           1  \n",
              "47688                           1  \n",
              "47689                           1  \n",
              "47690                           1  \n",
              "47691                           1  \n",
              "\n",
              "[47692 rows x 3 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# defining preprocess function\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "def preprocess(text):\n",
        "    text = text_lower(text)\n",
        "    text = text.apply(lambda text: clean_stopwords(text))\n",
        "    text = text.apply(lambda x : clean_puctuations(x))\n",
        "    text = text.apply(lambda x: clean_repeating_characters(x))\n",
        "    text = text.apply(lambda x : clean_URLs(x))\n",
        "    text = text.apply(lambda x: clean_numeric(x))\n",
        "    text = tokenize_tweet(text)\n",
        "    text = text.apply(lambda x: text_stemming(x))\n",
        "    text = text.apply(lambda x: text_lemmatization(x))\n",
        "    text = text.apply(lambda x : \" \".join(x))\n",
        "    return text\n",
        "\n",
        "data['tweet_text'] = preprocess(data['tweet_text'])\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "w8z6JlB7ayU3"
      },
      "outputs": [],
      "source": [
        "# Splitting the data into train and test\n",
        "X, y = data['tweet_text'], data['cyberbullying_type_encoded']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state= 41)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "RecSrsJJbF0y",
        "outputId": "810a1a7e-2130-4973-e6a5-34f558e5d9ea"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(max_features=500000, ngram_range=(1, 2))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=500000, ngram_range=(1, 2))</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "TfidfVectorizer(max_features=500000, ngram_range=(1, 2))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Transforming the data using TF-IDF Vectorizer\n",
        "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features= 500000)\n",
        "vectoriser.fit(X_train)\n",
        "# print(\"No. of feature words: \",len(vectoriser.get_feature_names()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hlR3K3zKbF2R"
      },
      "outputs": [],
      "source": [
        "# Dumping the vectoriser\n",
        "pickle.dump(vectoriser, open(r'C:\\Users\\DELL\\Desktop\\project\\savedModels\\tdf_vectorizer', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xDD5w2UzbNY6"
      },
      "outputs": [],
      "source": [
        "X_train = vectoriser.transform(X_train)\n",
        "X_test = vectoriser.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cul6ZrqybNak",
        "outputId": "bec148bf-506a-4357-da3a-f44fa98a614a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.99      0.95      2356\n",
            "           1       0.97      0.99      0.98      2466\n",
            "           2       0.89      0.87      0.88      2393\n",
            "           3       0.63      0.51      0.56      2402\n",
            "           4       0.60      0.65      0.62      2358\n",
            "           5       0.94      0.97      0.95      2333\n",
            "\n",
            "    accuracy                           0.83     14308\n",
            "   macro avg       0.82      0.83      0.82     14308\n",
            "weighted avg       0.82      0.83      0.82     14308\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Model\n",
        "model = SVC(kernel= 'linear', C = 1).fit(X_train, y_train)\n",
        "y_pred  = model.predict(X_test)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print the classification report\n",
        "print(report)\n",
        "\n",
        "\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo8d86BD_VTS",
        "outputId": "a454b617-72bc-42c1-862f-91550c8dc8c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8290466871680179"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yYEtJo4jondm"
      },
      "outputs": [],
      "source": [
        "pickle.dump(model,open(r'C:\\Users\\DELL\\Desktop\\project\\savedModels\\model.bin','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uqNQzcQyl2R1"
      },
      "outputs": [],
      "source": [
        "def custom_input_prediction(text):\n",
        "    import nltk\n",
        "    nltk.download('omw-1.4')\n",
        "    text = pd.Series(text)\n",
        "    text = preprocess(text)\n",
        "    text = [text[0],]\n",
        "    vectoriser = pickle.load(open(\"tdf_vectorizer\", \"rb\"))\n",
        "    text = vectoriser.transform(text)\n",
        "    model = pickle.load(open(\"model.bin\", \"rb\"))\n",
        "    prediction = model.predict(text)\n",
        "    prediction = prediction[0]\n",
        "\n",
        "    interpretations = {\n",
        "        0 : \"Age\",\n",
        "        1 : \"Ethnicity\",\n",
        "        2 : \"Gender\",\n",
        "        3 : \"Not Cyberbullying\",\n",
        "        4 : \"Other Cyberbullying\",\n",
        "        5 : \"Religion\"\n",
        "    }\n",
        "\n",
        "    for i in interpretations.keys():\n",
        "        if i == prediction:\n",
        "            return interpretations[i]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9nIz05CF4M1o"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading omw-1.4: <urlopen error [WinError 10060] A\n",
            "[nltk_data]     connection attempt failed because the connected party\n",
            "[nltk_data]     did not properly respond after a period of time, or\n",
            "[nltk_data]     established connection failed because connected host\n",
            "[nltk_data]     has failed to respond>\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'tdf_vectorizer'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m statement1 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMy Grandsons are angry about this gender free crap too! 2 in primary 2 @at high school T.he is 16 yr old ASD &amp; got bullied as did a girl in his SEN base. He had to step in as teachers to busy on phones playing games, wee lass would have had nowhere to run if loos unisex!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(custom_input_prediction(statement1))\n\u001b[0;32m      4\u001b[0m statement2 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mBut for u its Hinduphobia isnt it? When kashmiri pandits get killed, when a hindu girl gets raped by islamists, when radical islamic terrorism kill people in the world,u still keep quiet as if nothing is happening;but jump on when some1 says anything against islam!! #Hinduphobic\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(custom_input_prediction(statement2))\n",
            "Cell \u001b[1;32mIn[13], line 7\u001b[0m, in \u001b[0;36mcustom_input_prediction\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      5\u001b[0m text \u001b[39m=\u001b[39m preprocess(text)\n\u001b[0;32m      6\u001b[0m text \u001b[39m=\u001b[39m [text[\u001b[39m0\u001b[39m],]\n\u001b[1;32m----> 7\u001b[0m vectoriser \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mtdf_vectorizer\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m      8\u001b[0m text \u001b[39m=\u001b[39m vectoriser\u001b[39m.\u001b[39mtransform(text)\n\u001b[0;32m      9\u001b[0m model \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmodel.bin\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m))\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tdf_vectorizer'"
          ]
        }
      ],
      "source": [
        "statement1 = \"My Grandsons are angry about this gender free crap too! 2 in primary 2 @at high school T.he is 16 yr old ASD &amp; got bullied as did a girl in his SEN base. He had to step in as teachers to busy on phones playing games, wee lass would have had nowhere to run if loos unisex!\"\n",
        "print(custom_input_prediction(statement1))\n",
        "\n",
        "statement2 = \"But for u its Hinduphobia isnt it? When kashmiri pandits get killed, when a hindu girl gets raped by islamists, when radical islamic terrorism kill people in the world,u still keep quiet as if nothing is happening;but jump on when some1 says anything against islam!! #Hinduphobic\"\n",
        "print(custom_input_prediction(statement2))\n",
        "\n",
        "statement3 = \"There was certainly a more \"\"acceptable\"\" time for them to be made though in the eyes of our world at large (which also includes other jokes like rape, gaybashing, etc.) Shit, try watching Friends or Seinfeld and watch how many times they throw gay people under the bus for a laugh.\"\n",
        "print(custom_input_prediction(statement3))\n",
        "\n",
        "statement4 = \"I swear to God. This dumb nigger bitch. I have got to bleach my hair reeeeeal fuckin' soon. D:&lt; FUCK.\"\n",
        "print(custom_input_prediction(statement4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "# Define a dictionary to map encoded values to original labels\n",
        "label_map = {'0': 'age', '1': 'ethnicity', '2': 'gender', '3': 'not_cyberbullying', '4': 'other_cyberbullying', '5': 'religion'}  # Add more mappings as needed\n",
        "\n",
        "# Extract precision, recall, and F1-score from the report and convert to percentage\n",
        "labels = [key for key in report.keys() if key not in ['accuracy', 'macro avg', 'weighted avg']]\n",
        "precision = [report[key]['precision'] * 100 for key in labels]\n",
        "recall = [report[key]['recall'] * 100 for key in labels]\n",
        "f1_score = [report[key]['f1-score'] * 100 for key in labels]\n",
        "\n",
        "# Create a bar plot\n",
        "x = np.arange(len(labels))\n",
        "width = 0.2\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(x - width, precision, width, label='Precision')\n",
        "rects2 = ax.bar(x, recall, width, label='Recall')\n",
        "rects3 = ax.bar(x + width, f1_score, width, label='F1-score')\n",
        "\n",
        "# Add labels, title, and legend\n",
        "ax.set_ylabel('Scores (%)')\n",
        "ax.set_title('Classification Report Metrics')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([label_map[i] for i in labels])  # Use the original labels from the dictionary\n",
        "ax.legend()\n",
        "\n",
        "\n",
        "# Rotate x-axis labels by 45 degrees\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "ax.legend()\n",
        "\n",
        "# Format y-axis ticks as percentages\n",
        "ax.set_yticklabels(['{}%'.format(int(tick)) for tick in ax.get_yticks()])\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
